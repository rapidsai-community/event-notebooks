{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517bf1e7-5ade-4764-8a1b-58eade5455e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Evaluating Your Options for Numerical Computing in Pure Python: <br>Single Threaded CPU & Single GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93c13a-d939-45e6-894c-fece44399044",
   "metadata": {},
   "source": [
    "## **Prerequisites**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea43e1-e4bd-40ca-a2a3-f3fb0675c7e2",
   "metadata": {},
   "source": [
    "This tutorial assume proficiency in Python and the following libraries:\n",
    "\n",
    "* NumPy\n",
    "* Scikit-Learn\n",
    "* Numba\n",
    "\n",
    "Demo System - Benchmarking was performed on a DGX Station A100 320GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92137d74-37e6-4953-98ae-5704d4959f04",
   "metadata": {},
   "source": [
    "## **Problem Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d27c47-7597-4993-adc2-1dfee55124ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this notebook, we  survey techniques for numerical computing in pure Python. We leverage a proxy geospatial nearest neighbor problem to guide us through an evaluation of several libraries and methodologies. In this use case, we aim to resolve geospatial observations to their nearest reference points with an added complexity. Our complication adds dynamics to the problem allowing each reference point to move and the set of observations to change on a reoccurring basis. These complexities imply a need to recompute each nearest neighbor at each timestep -- emphasizing the need for high performance techiques. \n",
    "\n",
    "Because of its simplicity and arithmetic intensity, we focus our attention on the brute force nearest neighbor technique using the haversine great circle distance formula as our distance metric. This is a popular formula used to calculate the distance between two points on earth.\n",
    "\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/Haversine_formula\"><img src=\"./media/haversine-graphic.png\" alt=\"Haversine\" style=\"width: 150;\"></a></center></br>\n",
    "\n",
    "The graphic below illustrates the dynamic nature of our problem. From left to write, we can observe the dynamics of the system at each timestep -- with colored regions representing nearest neighbor decision boundaries for each reference point and points representing observations.\n",
    "\n",
    "<center><img src=\"./media/DynamicDecisionBoundaries.png\" alt=\"Visualization\" style=\"width: 1000;\"/></center>\n",
    "\n",
    "In this notebook, we will evalutate the following single threaded CPU techniques:\n",
    "\n",
    "* Conventional For Loop\n",
    "* NumPy Broadcasting\n",
    "* Scikit-Learn Brute Force KNN\n",
    "* Numba CPU Kernel\n",
    "\n",
    "In addition, we will evalutate several single GPU techniques:\n",
    "\n",
    "* CuPy Broadcasting\n",
    "* cuML Brute Force KNN\n",
    "* Hand tuned Numba CUDA Kernel\n",
    "\n",
    "In the end, we'll compare their performance on a moderate sized problem (defined below) and expand our numerical computing toolbox with a few new tricks.\n",
    "\n",
    "**Spoiler Alert -- The GPU techniques each out perform the CPU techniques by at least several orders of magnitude.**\n",
    "\n",
    "Because many of the CPU functions take so long, we use the ```%%time``` magic function and comment out ```%%timeit``` to generate benchmarks.\n",
    "\n",
    "Since many of the CPU techniques will take a very long time to complete, we provide an overview of the expected performance measured on a DGX Station A100.\n",
    "\n",
    "<center><img src=\"./media/AllModerateCpuGpuPerfTable.png\" alt=\"PerfTable\" style=\"width: 1250;\"/></center>\n",
    "\n",
    "Note - for completeness, multi-CPU and multi-GPU benchmarks are included in this table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f49802-ec33-454a-b8be-365427cda3c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Single Threaded CPU/Single GPU Experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c0741-9947-4332-97fe-2dc634461f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "from numba import (cuda, \n",
    "                   uint32, \n",
    "                   int32, \n",
    "                   float32,\n",
    "                   types, \n",
    "                   jit, \n",
    "                   prange)\n",
    "\n",
    "from cupyx.time import repeat\n",
    "from cuml.neighbors import NearestNeighbors as cuNearestNeighbors\n",
    "import rmm\n",
    "from src.simulator import generate_geos\n",
    "\n",
    "from src.utils import (query_available_memory, \n",
    "                       check_accuracy,\n",
    "                       check_accuracy_h2h)\n",
    "\n",
    "from math import sin, cos, sqrt, asin\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce7389-1696-4a04-8b12-e42fad6ba7df",
   "metadata": {},
   "source": [
    "Define constants for the size of our experiment and evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfec83-5a00-40c8-a454-f02184438b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS, N_REF = 2**17, 2**15 # single processor experiment\n",
    "N_OBS_VAL, N_REF_VAL = 500, 200 # check accuracy\n",
    "print(\"Problem Size (N_OBS * N_REF): {:.2f}B\".format(N_OBS * N_REF * 1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7d4ed-104d-4d8e-87d7-ac51dee59840",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **RAPIDS Memory Manager**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8289b-d489-4796-a55f-05811fc68c52",
   "metadata": {
    "tags": []
   },
   "source": [
    "The RAPIDS Memory Manager (RMM) provides us finer grained control over memory allocations. Each memory allocation strategy comes with performance and data movement considerations.\n",
    "\n",
    "Since our experiments will use a fair bit of GPU memory and we desire the fastest performance, let's reinitialize the RMM by pre-allocating all the free GPU memory on the device.\n",
    "\n",
    "Note: be sure to free up memory from other notebooks/applications to maximize resources for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea778ec9-9f8e-46c3-8a57-982bc87af1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_mem, device = query_available_memory()\n",
    "rmm.rmm.reinitialize(pool_allocator=True, initial_pool_size=free_mem, devices=device)\n",
    "cp.cuda.Device(device).use()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098f8e9-8c75-4311-b38b-6a1e9ea051d8",
   "metadata": {},
   "source": [
    "## **Generate Synthetic Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b373c17-d569-4d82-845d-1babd13fb035",
   "metadata": {},
   "source": [
    "Generate a moderate sized synthetic dataset and smaller validation dataset to run our experiments using an included utility function. These datasets represent the following:\n",
    "\n",
    "* ```d_obs``` contains ```N_OBS``` geospatial observations in radians on the GPU, used for our moderate scale benchmark\n",
    "* ```d_ref``` contains ```N_REF``` geospatial reference points in radians on the GPU, used for our moderate scale benchmark\n",
    "* ```d_obs_val``` contains ```N_OBS_VAL``` geospatial observations in radians on the GPU, used to validate accuracy\n",
    "* ```d_ref_val``` contains ```N_REF_VAL``` geospatial reference points in radians on the GPU, used to validate accuracy\n",
    "* ```h_``` prefixes represent copies of these data on the host to use with the CPU techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96559ac-9330-4c52-bc6b-6a5d33189fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ref = generate_geos(N_REF, random_state=1)\n",
    "d_obs = generate_geos(N_OBS, random_state=2)\n",
    "\n",
    "h_ref = d_ref.get()\n",
    "h_obs = d_obs.get()\n",
    "\n",
    "d_ref_val = generate_geos(N_REF_VAL, random_state=1)\n",
    "d_obs_val = generate_geos(N_OBS_VAL, random_state=2)\n",
    "\n",
    "h_ref_val = d_ref_val.get()\n",
    "h_obs_val = d_obs_val.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe363de-54d1-4956-b9bc-0659a9106973",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Naive Double For Loop**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e40f74-111c-4268-8707-a13d3c6967af",
   "metadata": {},
   "source": [
    "This first example demonstrates a vanilla double for loop, without any specialized libraries or techniques -- those unfamiliar with numerical computing libraries might choose something like this first:\n",
    "- Extremely straightforward implementation\n",
    "- Painfully slow ... (over 6hrs to complete on the demo system)\n",
    "- Probably what many people think when calling Python “slow”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9044d5-3bb3-4b60-93b4-b35114deb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_haversine(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    first_sin = sin((lat2 - lat1) / 2.)\n",
    "    second_sin = sin((lon2 - lon1) / 2.)\n",
    "    \n",
    "    a = first_sin * first_sin + \\\n",
    "        cos(lat1) * \\\n",
    "        cos(lat2) * \\\n",
    "        second_sin * second_sin\n",
    "    \n",
    "    a = sqrt(a)\n",
    "    \n",
    "    if a > 1.:\n",
    "        a = 1.\n",
    "    elif a < 0:\n",
    "        a = 0.\n",
    "        \n",
    "    a = asin(a)\n",
    "    \n",
    "    return 2.0 * a\n",
    "\n",
    "def loop_solve(a, b):\n",
    "    \n",
    "    out_idx = np.empty(\n",
    "        (a.shape[0]), dtype=np.uint32)\n",
    "    \n",
    "    out_dist = np.empty(\n",
    "        (a.shape[0]), dtype=np.float32)\n",
    "    \n",
    "    for obs_idx in range(a.shape[0]):\n",
    "        \n",
    "        glob_min_dist = 1e11\n",
    "        glob_min_idx = 0\n",
    "        \n",
    "        for ref_idx in range(b.shape[0]):\n",
    "            \n",
    "            temp_dist = loop_haversine(\n",
    "                a[obs_idx, 0],\n",
    "                a[obs_idx, 1],\n",
    "                b[ref_idx, 0],\n",
    "                b[ref_idx, 1])\n",
    "            \n",
    "            if temp_dist < glob_min_dist:\n",
    "                glob_min_dist = temp_dist\n",
    "                glob_min_idx = ref_idx\n",
    "        \n",
    "        out_dist[obs_idx] = glob_min_dist\n",
    "        out_idx[obs_idx] = glob_min_idx\n",
    "        \n",
    "    return out_idx, out_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb302393-8fcf-4eab-ab49-1bd6b907f72b",
   "metadata": {},
   "source": [
    "Now that we've written the functions, let's run it...actually, suggest preparing yourself for a very long wait time. Depending on your system, this will likely run for many hours (>6hrs on DGX Station A100 320GB) before completing. To spare your precious time, we've commented out the function and timing magic fuctions\n",
    "\n",
    "Note - we will use this technique as the basis to check accuracy of our alternative techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbd2a3-324c-429c-8b6d-b17dd489500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#%%timit\n",
    "#out_idx_loop, out_dist_loop = loop_solve(h_obs, h_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a6d3f-2d14-4681-9909-458bf555cf07",
   "metadata": {},
   "source": [
    "## **Broadcasting: NumPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b4654-b0e5-402f-a3a8-be426b284440",
   "metadata": {},
   "source": [
    "[Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) is implemented by [NumPy](https://numpy.org/doc/stable/index.html) and [CuPy](https://cupy.dev/) to efficiently deal with arrays of different shapes when performing arithmetic operations.\n",
    "Here, we we use numpy to vectorize our task by broadcasting our smaller array across the larger array to deal with the array size mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba28e49-3033-4bbb-aa03-152f6ccf4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_haversine(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    return 2.0 * np.arcsin(\n",
    "        np.sqrt(np.sin((lat2 - lat1) / 2.0)**2 + \\\n",
    "                np.cos(lat1) * \\\n",
    "                np.cos(lat2) * \\\n",
    "                np.sin((lon2 - lon1) / 2.0)**2)\n",
    "    )\n",
    "                \n",
    "def numpy_solve(a, b):\n",
    "    \n",
    "    a_broad = a[:,np.newaxis]\n",
    "    \n",
    "    temp = numpy_haversine(\n",
    "        a_broad[:,:,0],\n",
    "        a_broad[:,:,1],\n",
    "        b[:,0],\n",
    "        b[:,1]\n",
    "    )\n",
    "    \n",
    "    np.abs(temp, out=temp)\n",
    "    out_idx = temp.argmin(axis=1)\n",
    "    out_dist = temp[np.arange(a.shape[0]), out_idx]    \n",
    "    \n",
    "    return out_idx, out_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2094f-689f-4115-acfa-62782f8487ed",
   "metadata": {},
   "source": [
    "Verify our NumPy broadcasting function is producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4587de-d349-497d-b0df-24f6f81d7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx_np_val, out_dist_np_val = \\\n",
    "    numpy_solve(h_obs_val, h_ref_val)\n",
    "\n",
    "print(\"Accuracy - NumPy Single Threaded CPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, d_ref_val,\n",
    "          out_idx_np_val, out_dist_np_val)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f10cc4-2965-45ed-a135-17c962f19f66",
   "metadata": {},
   "source": [
    "This NumPy CPU routine completes on the demo system in ~1min23s -- 272x faster than our baseline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4881907-3743-4d90-97e4-a03ea39072b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%%timeit\n",
    "out_idx_np, out_dist_np = numpy_solve(h_obs, h_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d749a5-a005-4104-822e-d6f85c9a4d9c",
   "metadata": {},
   "source": [
    "## **Broadcasting: CuPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d931d30f-f6ab-4569-9241-8b5b3f825122",
   "metadata": {},
   "source": [
    "Next, we turn our attention to [CuPy](https://cupy.dev/), GPU-accelerated drop-in replacement library for NumPy. Here we apply our [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) technique with minimal syntax changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d639e8e-43c8-43da-b638-e50db71c5bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cupy_haversine(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    return 2.0 * cp.arcsin(\n",
    "        cp.sqrt(cp.sin((lat2 - lat1) / 2.0)**2 + \\\n",
    "                cp.cos(lat1) * \\\n",
    "                cp.cos(lat2) * \\\n",
    "                cp.sin((lon2 - lon1) / 2.0)**2)\n",
    "    )\n",
    "\n",
    "def cupy_solve(a, b):\n",
    "    \n",
    "    a_broad = a[:,cp.newaxis]\n",
    "    \n",
    "    temp = cupy_haversine(\n",
    "        a_broad[:,:,0],\n",
    "        a_broad[:,:,1],\n",
    "        b[:,0],\n",
    "        b[:,1]\n",
    "    )\n",
    "    \n",
    "    cp.abs(temp, out=temp, dtype=np.float32)\n",
    "    out_idx = temp.argmin(axis=1, dtype=np.int32)\n",
    "    out_dist = temp[cp.arange(a.shape[0]), out_idx]    \n",
    "    \n",
    "    return out_idx, out_dist  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417940d-e4cf-4877-966b-af3a8a61219c",
   "metadata": {},
   "source": [
    "Verify our CuPy GPU broadcasting function is producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42ce4e-b49b-436a-8239-d91765db8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx_cp_val, out_dist_cp_val = \\\n",
    "    cupy_solve(d_obs_val, d_ref_val)\n",
    "\n",
    "print(\"Accuracy - CuPy Single GPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, d_ref_val,\n",
    "          out_idx_cp_val, out_dist_cp_val)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912b601-736d-47ab-855c-1bd452be188d",
   "metadata": {},
   "source": [
    "This CuPy GPU routine completes on the demo system in ~686ms -- 32,886x faster than our baseline and 123x faster than the NumPy equivalent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0f724-c01c-4f15-9ed3-66aa2c781739",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "out_idx_cp, out_dist_cp = cupy_solve(d_obs, d_ref)\n",
    "cp.cuda.Device().synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8fab83-8378-44e1-828f-049043eb420d",
   "metadata": {},
   "source": [
    "Note - With both NumPy and CuPy, this broadcasting technique results in significant memory utilization during runtime -- roughly MxNx32bits + some overheads.\n",
    "This is not always an issue, but something worth knowing -- especially as the problem size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa704c-fff6-4c52-abf0-90887d59a972",
   "metadata": {},
   "source": [
    "## **Brute Force KNN: Scikit-learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94b484-1dc0-4246-8a4a-471a65852941",
   "metadata": {},
   "source": [
    "Here we leverage [scikit-learn's](https://scikit-learn.org/stable/) brute force [nearest neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors) implementation. Leaning on this library means much less code than the previous options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305340d-794c-4364-98e3-5f0656ece399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_knn_solve(a, b):\n",
    "    \n",
    "    knn = NearestNeighbors(\n",
    "        algorithm=\"brute\",\n",
    "        metric=\"haversine\")\n",
    "    \n",
    "    knn.fit(b)\n",
    "    \n",
    "    out_dist_sklrn, out_idx_sklrn = \\\n",
    "    knn.kneighbors(\n",
    "        a, \n",
    "        n_neighbors=1, \n",
    "        return_distance=True)\n",
    "    \n",
    "    return (out_idx_sklrn.reshape(a.shape[0]),\n",
    "            out_dist_sklrn.reshape(a.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92498301-d0bd-41c0-8b0d-3d0c2028eda9",
   "metadata": {},
   "source": [
    "Verify our scikit-learn function is producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c5883-feea-4d20-a926-004672d344bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx_sklearn_val, out_dist_sklearn_val = \\\n",
    "    sklearn_knn_solve(h_obs_val, h_ref_val)\n",
    "\n",
    "print(\"Accuracy - Sklearn Single Threaded CPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, d_ref_val,\n",
    "          out_idx_sklearn_val, out_dist_sklearn_val)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3686ad-86b1-4e24-b8ea-5f8d30e513a0",
   "metadata": {},
   "source": [
    "This scikit-learn CPU routine completes on the demo system in ~5min54s -- 64x faster than our baseline, but a little slower than our NumPy example. Good news is its more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112330d-c378-4986-9f82-0423a722dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%%timeit\n",
    "out_idx_sklearn, out_dist_sklearn = sklearn_knn_solve(h_obs, h_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625e010-7d53-41b2-a356-20ea5aa8d284",
   "metadata": {},
   "source": [
    "## **Brute Force KNN -- cuML**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d874bea-546c-4776-9ac0-e012a82277f7",
   "metadata": {},
   "source": [
    "Here we leverage the scikit-learn nearest neighbors [GPU-accelerated drop-in replacement](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.neighbors.NearestNeighbors) from [cuML](https://docs.rapids.ai/api/cuml/stable/), built on Facebook AI Similarity Search (FAISS).\n",
    "\n",
    "Again, notice the code syntax similarity. This is great news for scikit-learn developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0744e94-327e-46e4-860c-b20149206464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuml_knn_solve(a, b):\n",
    "    \n",
    "    cuknn = cuNearestNeighbors(\n",
    "        algorithm=\"brute\",\n",
    "        metric=\"haversine\")\n",
    "    \n",
    "    cuknn.fit(b)\n",
    "    \n",
    "    out_dist_cuml, out_idx_cuml = \\\n",
    "    cuknn.kneighbors(\n",
    "        a, \n",
    "        n_neighbors=1, \n",
    "        return_distance=True)\n",
    "    \n",
    "    return (out_idx_cuml.reshape(a.shape[0]),\n",
    "            out_dist_cuml.reshape(a.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037d944-8caf-4366-b974-a2eedf9402e0",
   "metadata": {},
   "source": [
    "Verify our cuML function is producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b597866-d0c2-4407-ae92-a0a195451616",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx_cuml_val, out_dist_cuml_val = \\\n",
    "    cuml_knn_solve(d_obs_val, d_ref_val)\n",
    "\n",
    "print(\"Accuracy - cuML Single GPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, d_ref_val,\n",
    "          out_idx_cuml_val, out_dist_cuml_val)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10565e-3e5f-4920-b7ee-f13497335452",
   "metadata": {},
   "source": [
    "This cuML GPU routine completes on the demo system in ~180ms -- 125,333x faster than our baseline and 1,967x faster then the scikit-learn equivalent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760ecc0-effa-413b-a9b5-2741e9dfaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "out_idx_cuml, out_dist_cuml = cuml_knn_solve(d_obs, d_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9020c7b1-c7c2-48ce-b645-744ad7d73cc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Numba CPU Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75907b16-d956-4bb6-91f8-805d64aefda5",
   "metadata": {},
   "source": [
    "[Numba](https://numba.pydata.org/) translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.\n",
    "\n",
    "Here, we leverage a Numba JIT compiled kernel based on our double for loop implementation and observe non-trivial speedups when compared to our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996def42-e080-43d3-a9e6-55d138d8e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, fastmath=True)\n",
    "def numba_cpu_haversine(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    return 2.0 * asin(\n",
    "        sqrt(sin((lat2 - lat1) / 2.0)**2 + \\\n",
    "             cos(lat1) * \\\n",
    "             cos(lat2) * \\\n",
    "             sin((lon2 - lon1) / 2.0)**2)\n",
    "    )      \n",
    "\n",
    "@jit(nopython=True)\n",
    "def numba_cpu_solve(a, b):\n",
    "    \n",
    "    out_idx = np.empty(\n",
    "        (a.shape[0]), dtype=np.uint32)\n",
    "    \n",
    "    out_dist = np.empty(\n",
    "        (a.shape[0]), dtype=np.float32)\n",
    "    \n",
    "    for obs_idx in range(a.shape[0]):\n",
    "        \n",
    "        glob_min_dist = 1e11\n",
    "        glob_min_idx = 0\n",
    "        \n",
    "        for ref_idx in range(b.shape[0]):\n",
    "            \n",
    "            temp_dist = numba_cpu_haversine(\n",
    "                a[obs_idx,0],\n",
    "                a[obs_idx, 1],\n",
    "                b[ref_idx, 0],\n",
    "                b[ref_idx, 1])\n",
    "            \n",
    "            if temp_dist < glob_min_dist:\n",
    "                glob_min_dist = temp_dist\n",
    "                glob_min_idx = ref_idx\n",
    "        \n",
    "        out_dist[obs_idx] = glob_min_dist\n",
    "        out_idx[obs_idx] = glob_min_idx\n",
    "        \n",
    "    return out_idx, out_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5c7a1-2814-49bb-a8a3-fea7eff815d3",
   "metadata": {},
   "source": [
    "Verify our Numba CPU kernel is producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c18337-6e85-4240-86f6-91cd38e5841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx_nb_cpu_val, out_dist_nb_cpu_val = \\\n",
    "    numba_cpu_solve(h_obs_val, h_ref_val)\n",
    "\n",
    "print(\"Accuracy - Numba Single Threaded CPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, d_ref_val,\n",
    "          out_idx_nb_cpu_val, out_dist_nb_cpu_val)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6b439-bb73-44e0-8852-39d42f8bbc86",
   "metadata": {},
   "source": [
    "This Numba CPU routine completes on the demo system in ~4min22s -- 86x faster than our baseline, but a little slower than our NumPy example. Good news is its more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb147f-7399-4c57-be61-b6800ed714ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%%timeit\n",
    "out_idx_nb_cpu, out_dist_nb_cpu = numba_cpu_solve(h_obs, h_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52a304-20a5-42b2-9155-d60fe1868da2",
   "metadata": {},
   "source": [
    "## **Numba CUDA Kernel (Advanced)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e015e-849a-41fb-a73c-c58b295368de",
   "metadata": {},
   "source": [
    "[Numba CUDA](https://numba.pydata.org/numba-doc/latest/cuda/index.html) provides us with an extremely pythonic interface to developing CUDA kernels without ever writing a single line of C/C++. It's pythonic syntax makes writing CUDA approachable to your typical data scientist. Numba CUDA exposes lower control to the developer including the use of shared memory, constant memory, atomics, warp level optimizations, and many others. Although not every element of CUDA is supported, developers are able to prototype highly performant algorithms very quickly. In many cases, these can be production ready techniques.\n",
    "\n",
    "Numba CUDA implements the CUDA Array Interface, which means GPU data structures are interoperable between CuPy, cuDF, and Deep Learning Frameworks (Tensorflow, PyTorch). This is compelling from an end to end data processing perspecitve. Keeping the data on GPU for as long as possible let's developers avoid bottlenecks that creep in from unnecessary data copies across the PCIe bus.\n",
    "\n",
    "Below, we implement several kernels to perform the nearest neighbor calculations. To demonstrate lower level CUDA support, our code performs a [sequential addressing tree based warp reduction](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/), leveraging efficient communcation between threads within the same warp.\n",
    "\n",
    "Do not worry if we lost you here. Some of these are advanced concepts. The key takeaway is that more sophisticated CUDA programming is available to you without ever leaving the land of Python.\n",
    "\n",
    "Learn more about the CUDA Programming Model [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9e97a-d3e7-4641-910d-a81a31db35ca",
   "metadata": {},
   "source": [
    "Here are our hand tuned Numba JIT CUDA kernels that deploy warp level optimization to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40e756-2e14-4a24-bab5-d1b004cb4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True, inline=True)\n",
    "def _warp_min_reduce_idx_unrolled(val, idx):\n",
    "    \n",
    "    mask  = 0xffffffff    \n",
    "        \n",
    "    shfl_val = cuda.shfl_down_sync(\n",
    "        mask, val, 16)\n",
    "    \n",
    "    shfl_idx = cuda.shfl_down_sync(\n",
    "        mask, idx, 16)\n",
    "\n",
    "    if val > shfl_val:\n",
    "        val = shfl_val\n",
    "        idx = shfl_idx\n",
    "        \n",
    "    shfl_val = cuda.shfl_down_sync(\n",
    "        mask, val, 8)\n",
    "    \n",
    "    shfl_idx = cuda.shfl_down_sync(\n",
    "        mask, idx, 8)\n",
    "\n",
    "    if val > shfl_val:\n",
    "        val = shfl_val\n",
    "        idx = shfl_idx        \n",
    "        \n",
    "    shfl_val = cuda.shfl_down_sync(\n",
    "        mask, val, 4)\n",
    "    \n",
    "    shfl_idx = cuda.shfl_down_sync(\n",
    "        mask, idx, 4)\n",
    "\n",
    "    if val > shfl_val:\n",
    "        val = shfl_val\n",
    "        idx = shfl_idx         \n",
    "        \n",
    "    shfl_val = cuda.shfl_down_sync(\n",
    "        mask, val, 2)\n",
    "    \n",
    "    shfl_idx = cuda.shfl_down_sync(\n",
    "        mask, idx, 2)\n",
    "\n",
    "    if val > shfl_val:\n",
    "        val = shfl_val\n",
    "        idx = shfl_idx         \n",
    "        \n",
    "    shfl_val = cuda.shfl_down_sync(\n",
    "        mask, val, 1)\n",
    "    \n",
    "    shfl_idx = cuda.shfl_down_sync(\n",
    "        mask, idx, 1)\n",
    "\n",
    "    if val > shfl_val:\n",
    "        val = shfl_val\n",
    "        idx = shfl_idx\n",
    "\n",
    "    return val, idx\n",
    "\n",
    "sig_block_get_nearest_brute = \"void(float32[:,:], float32[:,:], uint32[:,:], float32[:,:])\"\n",
    "def _block_get_nearest_brute(coord1, coord2, block_idx, block_dist):\n",
    "    \n",
    "    \"\"\"\n",
    "    GPU accelerated pairwise distance comparisons in single\n",
    "    precision.\n",
    "    \"\"\"    \n",
    "    \n",
    "    startx, starty = cuda.grid(2)\n",
    "    stridex, stridey = cuda.gridsize(2)\n",
    "    \n",
    "    seed = float32(1e11)\n",
    "        \n",
    "    for i in range(starty, coord2.shape[0], stridey):    \n",
    "        \n",
    "        b_min_val = seed\n",
    "        b_min_idx = uint32(0)\n",
    "        coord2_i_0 = coord2[i,0]\n",
    "        \n",
    "        for j in range(startx, coord1.shape[0], stridex):\n",
    "\n",
    "            coord1_j_0 = coord1[j,0]\n",
    "        \n",
    "            first_sin = sin(\n",
    "                (coord2_i_0 - coord1_j_0) * float32(0.5))\n",
    "            \n",
    "            second_sin = sin(\n",
    "                (coord2[i,1] - coord1[j, 1]) * float32(0.5))            \n",
    "\n",
    "            local_val = float32(2.0) * asin(\n",
    "                sqrt(\n",
    "                    first_sin * first_sin + \\\n",
    "                    cos(coord1_j_0) * \\\n",
    "                    cos(coord2_i_0) * \\\n",
    "                    second_sin * second_sin)\n",
    "            )            \n",
    "            \n",
    "            if local_val < b_min_val:\n",
    "                b_min_val = local_val\n",
    "                b_min_idx = j\n",
    "                \n",
    "                \n",
    "        b_min_val, b_min_idx = \\\n",
    "            _warp_min_reduce_idx_unrolled(\n",
    "            b_min_val, b_min_idx)\n",
    "\n",
    "        if cuda.laneid == 0:\n",
    "            block_dist[i, cuda.blockIdx.x] = b_min_val\n",
    "            block_idx[i, cuda.blockIdx.x] = b_min_idx\n",
    "            \n",
    "sig_global_get_nearest_brute = \"void(float32[:,:], uint32[:,:], float32[:], uint32[:])\"\n",
    "def _global_get_nearest_brute(block_dist, block_idx, out_dist, out_idx):        \n",
    "        \n",
    "    startx, starty = cuda.grid(2)\n",
    "    stridex, stridey = cuda.gridsize(2)\n",
    "    \n",
    "    seed = float32(1e30)\n",
    "    \n",
    "    for i in range(starty, out_dist.shape[0], stridey):\n",
    "        \n",
    "        g_min_dist = seed\n",
    "        g_min_idx = 0\n",
    "        \n",
    "        for j in range(startx, block_idx.shape[1], stridex):\n",
    "            \n",
    "            local_dist = block_dist[i, cuda.threadIdx.x]\n",
    "            \n",
    "            if local_dist < g_min_dist:\n",
    "                g_min_dist = local_dist\n",
    "                g_min_idx = block_idx[i, cuda.threadIdx.x]\n",
    "        \n",
    "        g_min_dist, g_min_idx = \\\n",
    "            _warp_min_reduce_idx_unrolled(\n",
    "            g_min_dist, g_min_idx)\n",
    "        \n",
    "        if cuda.laneid == 0:\n",
    "            out_dist[i] = g_min_dist\n",
    "            out_idx[i] = g_min_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd051e2-ff7d-4e9a-87ef-e583e8762074",
   "metadata": {},
   "source": [
    "Create our JIT kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2813a9d-8339-4b52-92cc-7a433a73a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_min_reduce = cuda.jit(\n",
    "    sig_block_get_nearest_brute,        \n",
    "    fastmath=True)(_block_get_nearest_brute)\n",
    "\n",
    "global_min_reduce = cuda.jit(\n",
    "    sig_global_get_nearest_brute,        \n",
    "    fastmath=True)(_global_get_nearest_brute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8cec4-8838-4f6a-965f-831b71edfdf3",
   "metadata": {},
   "source": [
    "### **Call our CUDA Kernel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aff5b4-a3c6-483a-be8b-e00e0347e04a",
   "metadata": {},
   "source": [
    "The function below solves our nearest neighbor problem in two steps. \n",
    "\n",
    "First, generate an intermediate solution by performing block level warp reductions by finding the closest 32 points for each data observation. The number 32 was selected to be equal to the size of a warp.\n",
    "\n",
    "We demonstrate interoperability with CuPy by passing in CuPy device arrays to our kernel.\n",
    "\n",
    "```block_min_reduce[bpg, tpb](d_ref, d_obs, d_block_idx, d_block_dist)```\n",
    "\n",
    "Our first launch configuration:\n",
    "\n",
    "* 2D grid of 3456 (a multiple of the # of GPU SMs) blocks -- 32x108\n",
    "* Blocks contain 2D arrangement of 512 (a multiple of 32) threads -- 32x16\n",
    "* Total of 1,769,472 threads\n",
    "\n",
    "We make a second kernel call to find our global solution. Since our previous kernel implements block level parallelism, a second kernel call helps us with a global synchronization. In this phase, we compute a global solution to our nearest neighbor problem.\n",
    "\n",
    "```global_min_reduce[bpg, tpb](d_block_dist, d_block_idx, d_out_dist, d_out_idx)```\n",
    "\n",
    "Our second launch configuration:\n",
    "\n",
    "* 1D grid of 108*20 blocks (multiple of the # of SMs)\n",
    "* Blocks contain 2D arrangement of 512 (a multiple of 32) -- 32x16\n",
    "* Total of 1,105,920 threads\n",
    "\n",
    "We make sure the CPU waits for processing to complete before continuing by performing a global synchronization with ```cuda.synchronize()```\n",
    "\n",
    "We achieve our final solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3733f80-97e0-4c49-936b-9d07d948e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numba_cuda_solve(d_obs, d_ref):\n",
    "    \n",
    "    d_out_idx = cuda.device_array((d_obs.shape[0],), dtype=np.uint32)\n",
    "    d_out_dist = cuda.device_array((d_obs.shape[0],), dtype=np.float32)     \n",
    "    \n",
    "    d_block_idx = cuda.device_array(\n",
    "        (d_out_idx.shape[0], 32), \n",
    "        dtype=np.uint32)\n",
    "\n",
    "    d_block_dist = cuda.device_array(\n",
    "        (d_out_idx.shape[0], 32), \n",
    "        dtype=np.float32)           \n",
    "\n",
    "    bpg = 32, 128\n",
    "    tpb = 32, 16\n",
    "\n",
    "    block_min_reduce[bpg, tpb](\n",
    "        d_ref, \n",
    "        d_obs, \n",
    "        d_block_idx,\n",
    "        d_block_dist)   \n",
    "\n",
    "    bpg = (1, 128*20)\n",
    "    tpb = (32, 16)        \n",
    "\n",
    "    global_min_reduce[bpg, tpb](\n",
    "        d_block_dist, \n",
    "        d_block_idx, \n",
    "        d_out_dist, \n",
    "        d_out_idx)   \n",
    "        \n",
    "    cuda.synchronize()\n",
    "    \n",
    "    return d_out_idx, d_out_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d8632-47d0-47c8-9b65-71a907fb6b04",
   "metadata": {},
   "source": [
    "Verify our Numba CUDA kernels are producing the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ecec6-0e2a-44f1-bebd-b031fbf0348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idx_nb_gpu_val, out_dist_nb_gpu_val = \\\n",
    "    numba_cuda_solve(d_obs_val, d_ref_val)\n",
    "\n",
    "print(\"Accuracy - Numba CUDA Single GPU:\", \n",
    "      check_accuracy(\n",
    "          d_obs_val, d_ref_val,\n",
    "          out_idx_nb_gpu_val, out_dist_nb_gpu_val)\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb3074-9b0d-475c-b895-7cda260ff157",
   "metadata": {},
   "source": [
    "This Numba CUDA kernels complete on the demo system in ~21.1ms -- mor than 1,000,000x faster than our baseline, and 3,860x faster than our most performant CPU option. In this scenario, Numba CUDA required more developer effort but provided us a framework to squeeze additional performance out of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed9bc8-3241-4869-9ecb-18448ad885f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "out_idx_nb_cuda, out_dist_nb_cuda = numba_cuda_solve(d_obs, d_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fba36-9d29-425a-b4d3-38041e28c1ea",
   "metadata": {},
   "source": [
    "# **Summarize the Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f0de1-428f-45f2-9ca4-84c1f83d5a2a",
   "metadata": {},
   "source": [
    "In summary, we have several options when it comes to single processor numerical computing in pure python.  We evaluated several options and observe the comparative performance below:\n",
    "\n",
    "<img src=\"./media/SingleModerateCpuGpuPerfTable.png\" alt=\"PerfTable\" style=\"width: 150;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232458cf-c7d5-4c00-94c5-229aaa6a26c9",
   "metadata": {},
   "source": [
    "We observe the library and processor selected to perform the computations has a clear impact on performance.\n",
    "\n",
    "This is particularly true when choosing GPUs as the compute workhorse -- achieving anywhere from hundred(s) to million\n",
    "times speedup when compared to the single CPU techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc667fda-c0fa-4f47-9b68-fe40f1e22c5a",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div align=\"left\"><h2><b>Please Restart the Kernel<b></h2></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810a2f4-857d-4f82-aba4-cbda6a3ba306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
