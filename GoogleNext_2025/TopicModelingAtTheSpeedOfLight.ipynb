{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rapidsai-community/event-notebooks/blob/main/GoogleNext_2025/TopicModelingAtTheSpeedOfLight.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixefs2cdQDmK"
   },
   "source": [
    "# Topic modeling at the speed of light\n",
    "\n",
    "Topic modeling is a type of statistical modeling used to discover abstract topics within a collection of documents. It is widely used in natural language processing (NLP) to uncover hidden thematic structures in large text. Traditional topic modeling techniques, such as Latent Dirichlet Allocation (LDA), can be computationally intensive, especially with large datasets. Leveraging GPUs can significantly accelerate the process, making it feasible to handle larger datasets and more complex models.\n",
    "\n",
    "## Why would you use GPUs?\n",
    "\n",
    "GPUs (Graphics Processing Units) are designed to handle parallel processing tasks efficiently. They are particularly well-suited for the matrix and vector operations that are common in machine learning and deep learning algorithms. By utilizing GPUs, we can achieve substantial speedups in training and inference times for topic modeling.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9c5Jh3_QXZY"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's make sure we are running on an runtime with a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZ6PU5y9RVo-",
    "outputId": "0d70d402-8103-42f7-a4c1-bf14facbdf9c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPk6oYL4Q2HG"
   },
   "source": [
    "If the above cell returns an error please make sure that you use a container or runtime with a GPU attached and restart.\n",
    "\n",
    "To perform the topic modeling we will us [BERTopic](https://maartengr.github.io/BERTopic/index.html), a widely used NLP (Natural Language Processing) framework built on top of BERT embeddings and designed to provide coherent and naturally sounding topic descriptions. So, let's make sure the package is available in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIWsX5kESl3X"
   },
   "outputs": [],
   "source": [
    "!pip install bertopic --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qn_C5TLSN3W"
   },
   "source": [
    "### Imports\n",
    "\n",
    "RAPIDS provides a new experience that allows you to harness the capabilities of GPUs to run your code authored in pandas or scikit-learn, all *without* the need to change your code in a meaningful way. The Zero-Code-Change (ZCC) experience runs seamlessly on a GPU without doing any additional work on the user's part. And in any case the code to be run on a GPU has not been yet supported, the framwork will then execute the CPU version of the code without any input from the user!\n",
    "\n",
    "![test](https://rapids.ai/cudf-pandas/chart.png)\n",
    "\n",
    "To enable this experience, all you need to to is to add these lines on top of your script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYZgmf8mRcc_",
    "outputId": "36688eea-923c-4ede-bed0-364572ba0ed1"
   },
   "outputs": [],
   "source": [
    "%load_ext cudf.pandas\n",
    "%load_ext cuml.experimental.accel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe2uHKQ_UI_Q"
   },
   "source": [
    "Now that we have the environment set up, we can do our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gHJWbVvSHSw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2no0ziOUPBH"
   },
   "source": [
    "We are using a bunch of frameworks here. Most of these are fairly selfexplanatory (who doesn't know pandas?!) and we have already touched upon BERTopic. The remaining frameworks help us with the following:\n",
    "\n",
    "1. [SentenceTransformer](https://www.sbert.net/) is a part of a large collection of over 5000 pre-trained models that help create embeddings we will use to train the topic modeling model.\n",
    "2. [UMAP](https://umap-learn.readthedocs.io/en/latest/) is a STOA dimensionality reduction tool that is useful with non-linear problems.\n",
    "3. [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) is a powerful clustering algorithm that uses density-based algorithm (DBSCAN) to find clusters but was further extended to convert it into a hierchical version (hence the HD prefix in the name).\n",
    "\n",
    "## Download the data\n",
    "In this example we will be using a Amazon Review dataset and focus on the reviews of beauty products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9A6DcMTSOM1",
    "outputId": "e83c95d3-d363-4d17-8a4b-c40c4953d96b"
   },
   "outputs": [],
   "source": [
    "!wget https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/All_Beauty.jsonl.gz --no-check-certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3KTIjP1WpQN"
   },
   "source": [
    "The code downloads to a local drive so now we can use pandas like we normally would but all this code actually runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTS9n6PSTUqj"
   },
   "outputs": [],
   "source": [
    "path = \"All_Beauty.jsonl.gz\"\n",
    "data = pd.read_json(path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xcsLV1NW5_v"
   },
   "source": [
    "You can check this for yourself by running the `nvidia-smi` command and you should see about 1GB memory usage on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLqw3vGrW-Xk",
    "outputId": "8a2dd42e-74d6-4a7f-9ca8-aae3e7e2763d"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jqiK33YXJMX"
   },
   "source": [
    "In this particular exercise -- we will only use the first 200k records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeblFoLUTlFl"
   },
   "outputs": [],
   "source": [
    "# Limit to e.g., 200K records for demo purposes\n",
    "N = 200000\n",
    "\n",
    "data = data.head(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIatxpftXZ0X"
   },
   "source": [
    "Let's have a peek what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "G4wi4ySnXd7k",
    "outputId": "5b6e5089-a874-4e36-a631-0281a54676ac"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X33kkSiSXO-6"
   },
   "source": [
    "We have the rating and additional metadata associated with the review. However, we will be using the `text` column only as we are interested in understanding if we can uncover any patterns in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_Tfpfv_8X7db",
    "outputId": "f12265c1-e912-48ec-f5e8-2c2a9cf38780"
   },
   "outputs": [],
   "source": [
    "sample_docs = data.text.tolist()\n",
    "sample_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5kD_Fy4X-C1"
   },
   "source": [
    "## Let's have some fun!\n",
    "\n",
    "Now that we have the data to work with -- let's start our main task: the topic modeling. First, we cannot simply pass text to the BERTopic model and we need to turn each and every sentence into a numerical representation -- an embedding. In this notebook we will use the [`all-MiniLM-L6-v2`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "8b4f0efb9eda4b4fb84e143ab406cfd3",
      "96032e32ad9548c3aea69d986da0cc46",
      "7fd74ffc6f084cd0840936956e66b662",
      "3e07c6fb80a645b9b5da60be0d52e46f",
      "fc40878bc93a4918ba1ad96b46167d46",
      "1ca5215068ef43b2beff6dfd7ec6f491",
      "0f0c82e227b54382976f5fe2c1c789f0",
      "79e4b28aa96f49e4bee7f9c89d841e9a",
      "1ca1b595228a4690a11b4260b168ee43",
      "f36814587b704097a9077f7db6ba23c0",
      "a76685dd740e41b7a92b2ba76c2143f7"
     ]
    },
    "id": "3Hjq5WaCTpe0",
    "outputId": "0182516c-4873-4df2-c3b1-dc18bc3c1430"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(sample_docs, batch_size=128, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my8Dw0ziYoiK"
   },
   "source": [
    "This process may take a 2 minutes or so but it's a process we only need to do once. Next, now that we have the embeddings, we can train our initial *vanilla* topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaYi0quqT0aq",
    "outputId": "3a96f2e3-415b-40cb-d910-b522ef58f799"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(sample_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDaImfCWZLud"
   },
   "source": [
    "Woot! We have now successfully trained a BERTopic model! On a GPU nonetheless!\n",
    "\n",
    "Let's explore what we have learned! First, we can quickly discern the most commonly occuring words in each topic (here we only use 8 top topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "feuTkMaxT4Oi",
    "outputId": "11de5b34-c2db-4778-fa97-fe47f7fd9424"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jQBG4j7Zyi8"
   },
   "source": [
    "So we can clearly see that for each topic -- we see semantically related words. This is good!\n",
    "\n",
    "But how many topics there are, you ask!? Well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIvQ199BZxoP",
    "outputId": "8a8a3d8f-f2ef-446c-a405-8e2e6fba179b"
   },
   "outputs": [],
   "source": [
    "print(f'The model idenitified {len(set(topics))} distinct topics...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2dMgMCmaO44"
   },
   "source": [
    "That's a lot... Let's see a distribution of how common each topic was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "cxRuFNW5aYIr",
    "outputId": "a871b42b-473d-4826-ef86-b03d8bd81151"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(topics).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayiJnX2mam05"
   },
   "source": [
    "Well... we're seeing that it's a long tail distribution and likely we can do better than this. Let's see if we're seeing any similarity between these topics before we proceed to refine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "IMpdc47WUvOe",
    "outputId": "ddeb5f09-6d53-48a9-a93a-8f39b6e5cdb0"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(top_n_topics=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlKxWtufa6lx"
   },
   "source": [
    "So the heatmap clearly shows *regions* of similar / overlapping topics (the more blue areas) and patches of less condensed overlap. This is likely better visible on the distance map between topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "BlfywJR9Uzqq",
    "outputId": "38789901-c591-4d75-e3f5-f26b9c2b3e7d"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyBMg1I4bT7D"
   },
   "source": [
    "Okay... There's a lot of topics but **many** overlaps!! We can surely do better than that now that we have this knowledge!\n",
    "\n",
    "## Clustering to the rescue!\n",
    "\n",
    "We can use the UMAP to reduce the dimensionality of our dataset and then apply a clustering model (the HDBSCAN) to semantically (since we're working on embeddings!) group some of the reviews into more refined clusters!\n",
    "\n",
    "And the added benefit -- it all runs on a GPU!!! So no more waiting long time for the UMAP model alone to finish it's job! Running on a GPU gives us the freedom to experiment at a lightning speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "l5OC6xHhU4Wy",
    "outputId": "401474e3-3423-446f-cc87-1cdc1ee68b14"
   },
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_components=15, n_neighbors=15, min_dist=0.0)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=100, gen_min_span_tree=True, prediction_data=True)\n",
    "\n",
    "topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "%time topics, probs = topic_model.fit_transform(sample_docs, embeddings)\n",
    "\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUyZVCffcJAx"
   },
   "source": [
    "As I was saying... it's quick!\n",
    "\n",
    "And now we have a much better refined topics (and there's only 105 of them!) Let's see if we still have similar words retained in each topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2GRUXx1OXVpx",
    "outputId": "0273768b-d635-4c50-9645-72d8147ab3e3"
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjBN9oDvcyUY"
   },
   "source": [
    "Unsurprisingly -- we still do. There are still some topics that could be closely related and viewed as related e.g. topic 0 and topic 6 could be related in some cases when the reviewer talks about how gentle certain shampoos or conditioners are to the skin and how nicely they smell.\n",
    "\n",
    "Luckily, we used the HDBSCAN and we can quickly pull up these hierarchies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "otIRf1P0PWOV",
    "outputId": "9ee991a2-305f-4d32-9fee-bf592b961c48"
   },
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(sample_docs)\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZC06scOeLwk"
   },
   "source": [
    "Nice. So now we can further decide how to further cluster our topics. However, one thing that we saw when we first peeked inside the dataset that there was a ton of short reviews. These reviews are likely skewing our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_OgXS7fQejQo",
    "outputId": "94e40c67-8f99-4c33-d7c1-832e2b72f7ef"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeLfNvRFemmN"
   },
   "source": [
    "Let's see how big of a problem this is for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "x-0e9MfXeq90",
    "outputId": "6c7c5889-202f-4cb1-c5bb-d5ff6b262de3"
   },
   "outputs": [],
   "source": [
    "data_word_count = data.text.str.count(\"\\w+\")\n",
    "data_word_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-mJ7dAKe4XC"
   },
   "source": [
    "Now let's see the stats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "YSs4xU_Ke72Q",
    "outputId": "825ae645-6134-4420-831c-5e0f9394471a"
   },
   "outputs": [],
   "source": [
    "data_word_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsYmNCdefCih"
   },
   "source": [
    "Alright... So, we see that the median is 21 words - that's a decent length. However, we'd be filtering out half of our reviews. I think 10 words (i.e. over 70% of our dataset) would be usable so let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FL8bCSUbPu1a",
    "outputId": "79ff2357-964b-47d4-aab7-a31323d21c91"
   },
   "outputs": [],
   "source": [
    "longer_reviews = data.loc[\n",
    "    data.text.str.count(\"\\w+\") >= 10\n",
    "]\n",
    "\n",
    "filtered_docs = longer_reviews.text.tolist()\n",
    "filtered_embeddings = embeddings[longer_reviews.index]\n",
    "\n",
    "filtered_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nX4BSZDfpAy"
   },
   "source": [
    "Good! We still have almost 150k reviews left but we are now guaranteed that these reviews convey a little bit more information than a simple -- \"good product\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y07ZMzlHfnI_",
    "outputId": "0ecd27cd-3a70-457f-c55b-83e393722586"
   },
   "outputs": [],
   "source": [
    "topic_model_long = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "%time topics, probs = topic_model_long.fit_transform(filtered_docs, filtered_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ci9qB07ggmR"
   },
   "source": [
    "How many topics we're seeing now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqCJ2TUAgKIo",
    "outputId": "460e6f7d-f018-467c-922f-d37be45f90f7"
   },
   "outputs": [],
   "source": [
    "print(f'Revised number of topics: {len(set(topics))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "dKsdBfW6glie",
    "outputId": "de9f197e-ffd9-4e25-c413-b04ffc7b9150"
   },
   "outputs": [],
   "source": [
    "topic_model_long.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5PHjaiihfiU"
   },
   "source": [
    "Okay, this looks even better than before! Let's check the word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kj1S3HwCgzaU",
    "outputId": "dcfd3768-9ff3-410b-ad8a-fa7802e91fec"
   },
   "outputs": [],
   "source": [
    "topic_model_long.visualize_barchart(top_n_topics=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivCk5gmih0Ua"
   },
   "source": [
    "# Summary\n",
    "\n",
    "In the end, in less than a few minutes, we have a well defined clusters of topics that the reviewers of beauty products cared to share with us. Thanks to the power of GPU we were able to quickly sift through 200k reviews and come up with up to 70 clearly delineated topics that using HDBSCAN we can further group into logical topics.\n",
    "\n",
    "And we were able to achieve *all* of this without changing any code that would have run for hours on a CPU.\n",
    "\n",
    "The power of GPUs gives you the power to build better models faster!\n",
    "\n",
    "Try it out for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvJugDmnixcK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f0c82e227b54382976f5fe2c1c789f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ca1b595228a4690a11b4260b168ee43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ca5215068ef43b2beff6dfd7ec6f491": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e07c6fb80a645b9b5da60be0d52e46f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f36814587b704097a9077f7db6ba23c0",
      "placeholder": "​",
      "style": "IPY_MODEL_a76685dd740e41b7a92b2ba76c2143f7",
      "value": " 1563/1563 [02:11&lt;00:00, 87.56it/s]"
     }
    },
    "79e4b28aa96f49e4bee7f9c89d841e9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fd74ffc6f084cd0840936956e66b662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79e4b28aa96f49e4bee7f9c89d841e9a",
      "max": 1563,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ca1b595228a4690a11b4260b168ee43",
      "value": 1563
     }
    },
    "8b4f0efb9eda4b4fb84e143ab406cfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_96032e32ad9548c3aea69d986da0cc46",
       "IPY_MODEL_7fd74ffc6f084cd0840936956e66b662",
       "IPY_MODEL_3e07c6fb80a645b9b5da60be0d52e46f"
      ],
      "layout": "IPY_MODEL_fc40878bc93a4918ba1ad96b46167d46"
     }
    },
    "96032e32ad9548c3aea69d986da0cc46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ca5215068ef43b2beff6dfd7ec6f491",
      "placeholder": "​",
      "style": "IPY_MODEL_0f0c82e227b54382976f5fe2c1c789f0",
      "value": "Batches: 100%"
     }
    },
    "a76685dd740e41b7a92b2ba76c2143f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f36814587b704097a9077f7db6ba23c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc40878bc93a4918ba1ad96b46167d46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
